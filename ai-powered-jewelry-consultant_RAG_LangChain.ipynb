{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "authors": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building a RAG System for Your Project\n",
    "\n",
    "**Goal:** To create a virtual consultant that can answer customer questions, provide consultation on jewelry care, material descriptions, and help select jewelry based on customer preferences.\n",
    "\n",
    "To create an intelligent assistant for a jewelry store that will provide not only general answers to questions but also consult on more specific information (for example, on materials, jewelry care, price determination, selection recommendations, etc.), **Retrieval-Augmented Generation (RAG)** can be applied. RAG is an approach that combines text generation and information retrieval, allowing for effective answering of questions using both the model's knowledge and external data sources (e.g., databases).\n",
    "\n",
    "In this brief guide, we will create a simple RAG system using an LLM from Meta AI - **Llama 3**, specifically **IlyaGusev/saiga_llama3_8b**, fine-tuned for the Russian language, [link](https://huggingface.co/IlyaGusev/saiga_llama3_8b) to the Hugging Face Hub. To work with unstructured data, we will use the **Unstructured Serverless API** We will use **LangChain** to build the action chain. We will also consider the following databases:\n",
    "- **FAISS** for the vector store\n",
    "- **ChromaDB**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredAPIFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.cache import GPTCache\n",
    "\n",
    "# Set the API key for Unstructured\n",
    "# An API key is required to work with different file formats - [Unstructured API key]\n",
    "os.environ[\"UNSTRUCTURED_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/content/drive/MyDrive/data\"\n",
    "os.makedirs(directory_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Faiss\n",
    "\n",
    "### Creating a FAISS vector store\n",
    "\n",
    "**Description:** FAISS is a library designed for efficient searching and clustering in large vector datasets. It is optimized for fast nearest neighbor search (NNS)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/data/\"\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "def load_and_split_documents(path, file_name, text_splitter):\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    try:\n",
    "        loader = UnstructuredAPIFileLoader(file_path)\n",
    "        data = loader.load()\n",
    "        docs = text_splitter.split_documents(data)\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing {file_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_and_decode(file_path):\n",
    "    # Function to load files and decode text\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except UnicodeDecodeError:\n",
    "        # Attempt to decode if the text is initially encoded\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                text = f.read().encode('latin-1').decode('utf-8')\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding file {file_path}: {e}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create a second database with product information\n",
    "def create_product_documents(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        products = json.load(f)\n",
    "    \n",
    "    product_docs = []\n",
    "    for product in products:\n",
    "        product_string = (\n",
    "            f\"Name: {product['name']}. \"\n",
    "            f\"Description: {product['description']}. \"\n",
    "            f\"Purpose: {product['purpose']}. \"\n",
    "            f\"Material: {product['material']}. \"\n",
    "            f\"Price: {product['price']} rubles. \"\n",
    "            f\"Link: {product['url']}\"\n",
    "        )\n",
    "        # Converting elements into Document objects\n",
    "        doc = Document(\n",
    "            page_content=product_string,\n",
    "            metadata={\n",
    "                \"name\": product['name'],\n",
    "                \"description\": product['description'],\n",
    "                \"price\": product['price'],\n",
    "                \"url\": product['url']\n",
    "            }\n",
    "        )\n",
    "        product_docs.append(doc)\n",
    "    return product_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parsing Input Documents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Loading elements with decoded text\n",
    "file_paths = [\n",
    "    os.path.join(path, \"уход.txt\"),\n",
    "    os.path.join(path, \"уход.docx\"),\n",
    "    os.path.join(path, \"носка.docx\"),\n",
    "    os.path.join(path, \"на_что_обратить_внимание_при_выборе_ювелирных_изделий.pdf\")\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "for file_name in [\"уход.docx\", \"носка.docx\", \"на_что_обратить_внимание_при_выборе_ювелирных_изделий.pdf\"]:\n",
    "    all_docs.extend(load_and_split_documents(path, file_name, text_splitter))\n",
    "\n",
    "# Load and decode the text file separately\n",
    "text_content = load_and_decode(os.path.join(path, \"уход.txt\"))\n",
    "if text_content:\n",
    "    text_docs = text_splitter.split_text(text_content)\n",
    "    for t in text_docs:\n",
    "        all_docs.append(Document(page_content=t, metadata={}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 26\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents loaded: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_faiss = FAISS.from_documents(all_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a second database with product information"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of catalog data\n",
    "catalog_data = [\n",
    "    {\n",
    "        \"name\": \"Diamond Ring\",\n",
    "        \"description\": \"Elegant ring with a high-quality 0.5-carat diamond.\",\n",
    "        \"purpose\": \"Perfect for special occasions, such as engagements or weddings.\",\n",
    "        \"material\": \"White gold 585\",\n",
    "        \"price\": 120000,\n",
    "        \"url\": \"https://example.com/ring1\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Silver Bracelet\",\n",
    "        \"description\": \"Classic silver bracelet, suitable for daily wear.\",\n",
    "        \"purpose\": \"Daily wear, adding a stylish accent.\",\n",
    "        \"material\": \"Silver 925\",\n",
    "        \"price\": 5000,\n",
    "        \"url\": \"https://example.com/bracelet1\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Gold Earrings with Sapphires\",\n",
    "        \"description\": \"Refined gold earrings with natural sapphires.\",\n",
    "        \"purpose\": \"Gift for a loved one, evening events.\",\n",
    "        \"material\": \"Yellow gold 750, sapphires\",\n",
    "        \"price\": 45000,\n",
    "        \"url\": \"https://example.com/earrings1\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Saving data to a JSON file\n",
    "with open(os.path.join(path, \"catalog.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(catalog_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example of loading the catalog\n",
    "product_docs = create_product_documents(os.path.join(path, \"catalog.json\"))\n",
    "\n",
    "db_faiss_products = FAISS.from_documents(product_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving and loading DB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS database saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to save the database\n",
    "db_faiss.save_local(os.path.join(path, \"faiss_index_care\"))\n",
    "db_faiss_products.save_local(os.path.join(path, \"faiss_index_products\"))\n",
    "print(\"FAISS database saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS database loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "loaded_db_care = FAISS.load_local(os.path.join(path, \"faiss_index_care\"), embeddings)\n",
    "loaded_db_products = FAISS.load_local(os.path.join(path, \"faiss_index_products\"), embeddings)\n",
    "print(\"FAISS database loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%\n",
      "Loading checkpoint shards: 100%\n"
     ]
    }
   ],
   "source": [
    "model_id = \"IlyaGusev/saiga_llama3_8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostGenerator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # Function for text generation\n",
    "    def generate_text(self, system_prompt, user_prompt):\n",
    "        # Tokenize the input\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # Text generation\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            eos_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        # Decode the generated tokens into text\n",
    "        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Remove the input prompt from the result\n",
    "        response = result.split('<|end_header_id|>')[-1].strip().split('user')[-1].strip().split('<|eot|>')[-2].strip().split('\\n\\n')[-1].strip()\n",
    "        return response\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "generator = PostGenerator(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at generation without RAG"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Tell me about how to care for silver jewelry\n",
      "Привет! Серебро — это прекрасный и благородный металл, но со временем он может потускнеть. Чтобы ваши серебряные украшения всегда выглядели как новые, следуйте простым правилам ухода:\n",
      "\n",
      "1. **Хранение:** Храните серебро отдельно от других украшений, в сухом и темном месте, желательно в специальных мешочках или шкатулках. Избегайте попадания прямых солнечных лучей.\n",
      "2. **Контакт с химикатами:** Снимайте серебряные украшения перед контактом с бытовой химией, косметикой, парфюмом, хлорированной водой (бассейн).\n",
      "3. **Чистка:** Регулярно чистите серебро мягкой тряпочкой. Для более глубокой чистки используйте специальные средства для серебра или народные методы, например, слабый раствор соды или нашатырного спирта. Важно не использовать абразивные средства, которые могут поцарапать поверхность.\n",
      "4. **Носка:** Носите серебро чаще! Контакт с естественными жирами кожи может предотвратить потемнение. Однако, если вы занимаетесь спортом или выполняете тяжелую физическую работу, лучше снять украшения.\n",
      "\n",
      "Следуя этим простым советам, вы сможете сохранить блеск и красоту ваших серебряных украшений на долгие годы. Если у вас есть украшения с драгоценными камнями, уточните особенности ухода, так как некоторые камни (например, жемчуг, опал) требуют более деликатного обращения.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"Ты умный ассистент, который дает советы по уходу за ювелирными украшениями. Твой ответ должен быть дружелюбным, информативным и лаконичным.\"\n",
    "user_prompt = \"Привет! Расскажи о том как ухаживать за серебрянными украшениями\"\n",
    "\n",
    "print(user_prompt)\n",
    "\n",
    "# Pipeline for text generation\n",
    "output = generator.generate_text(system_prompt, user_prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ChromaDB\n",
    "\n",
    "**ChromaDB** is a modern vector database that is mainly used for RAG (Retrieval-Augmented Generation) systems. It combines vector and metadata management.\n",
    "\n",
    "### Creating a ChromaDB Vector Store\n",
    "\n",
    "Let's create two collections. The first stores information with instructions for jewelry care. The second contains product information - the catalog."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents successfully added to the collection.\n",
      "Documents successfully added to the collection.\n"
     ]
    }
   ],
   "source": [
    "db_chroma_care = Chroma.from_documents(documents=all_docs, \n",
    "                                      embedding=embeddings, \n",
    "                                      collection_name=\"jewelry_care_collection\", \n",
    "                                      persist_directory=os.path.join(path, \"chroma_db_care\"))\n",
    "\n",
    "db_chroma_products = Chroma.from_documents(documents=product_docs, \n",
    "                                          embedding=embeddings, \n",
    "                                          collection_name=\"jewelry_catalog_collection\",\n",
    "                                          persist_directory=os.path.join(path, \"chroma_db_products\"))\n",
    "print(\"Documents successfully added to the collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Database Reuse"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'jewelry_catalog_collection' successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Initializing the client with the same path to load the saved database\n",
    "client_path = os.path.join(path, \"chroma_db_products\")\n",
    "collection_name = \"jewelry_catalog_collection\"\n",
    "\n",
    "try:\n",
    "    # Checking if the collection exists and loading it\n",
    "    loaded_db_chroma_products = Chroma(persist_directory=client_path, \n",
    "                                       embedding_function=embeddings, \n",
    "                                       collection_name=collection_name)\n",
    "    \n",
    "    # Getting only the collection names\n",
    "    if loaded_db_chroma_products._collection.count() > 0:\n",
    "        print(f\"Collection '{collection_name}' successfully loaded.\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' not found.\") # Placeholder for invalid data\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading ChromaDB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading Data from DB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents from the collection: ['Name: Diamond Ring. Description: Elegant ring with a high-quality 0.5-carat diamond. Purpose: Perfect for special occasions, such as engagements or weddings. Material: White gold 585. Price: 120000 rubles. Link: https://example.com/ring1', 'Name: Silver Bracelet. Description: Classic silver bracelet, suitable for daily wear. Purpose: Daily wear, adding a stylish accent. Material: Silver 925. Price: 5000 rubles. Link: https://example.com/bracelet1', 'Name: Gold Earrings with Sapphires. Description: Refined gold earrings with natural sapphires. Purpose: Gift for a loved one, evening events. Material: Yellow gold 750, sapphires. Price: 45000 rubles. Link: https://example.com/earrings1']\n"
     ]
    }
   ],
   "source": [
    "# Assuming the collection is successfully loaded\n",
    "results = loaded_db_chroma_products._collection.get(include=['documents', 'metadatas'])\n",
    "\n",
    "# This is a list of all texts/documents in the collection\n",
    "all_texts = results['documents'] \n",
    "print(\"All documents from the collection:\", all_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating LLM and RAG Chain"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LLM via HuggingFacePipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Function to search for text in collections\n",
    "def extract(user_query):\n",
    "    care_docs = loaded_db_care.similarity_search(user_query, k=3)\n",
    "    product_docs = loaded_db_products.similarity_search(user_query, k=3)\n",
    "    \n",
    "    care_context = \"\\n---\\n\".join([doc.page_content for doc in care_docs])\n",
    "    \n",
    "    product_context = []\n",
    "    for doc in product_docs:\n",
    "        product_context.append(\n",
    "            f\"Name: {doc.metadata['name']}\\nDescription: {doc.page_content}\\nPrice: {doc.metadata['price']} rubles\\nLink: {doc.metadata['url']}\\n\"\n",
    "        )\n",
    "    product_context_str = \"\\n---\\n\".join(product_context)\n",
    "\n",
    "    return product_context_str, care_context\n",
    "\n",
    "# Template for query formation\n",
    "template = (\n",
    "    \"You are a smart assistant specializing in jewelry. Your main tasks are: \"\n",
    "    \"1. Answer questions about jewelry, their characteristics, and prices based on the following context {context} \"\n",
    "    \"2. Help clients choose suitable products based on the following context {products} \"\n",
    "    \"Your goal is to provide helpful, clear, and friendly answers. If you don't know the answer, just say: «I don't know». Do not invent information. When suggesting products, try to be specific and describe how the product can help. If more information is needed for an answer, ask clarifying questions. \"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"products\", \"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Initializing the semantic cache.\n",
    "set_llm_cache(GPTCache(\n",
    "    similarity_threshold=0.9, # Euclidean distance threshold for determining question similarity.\n",
    "                              # a distance of 0 means the sentences are identical\n",
    "                              # We only return sentences from the cache below this threshold value\n",
    "    max_size=1000, # Maximum number of responses that the cache can store.\n",
    "    eviction_policy=None # Policy for evicting elements from the cache.\n",
    "                         # This can be any policy, for example, 'FIFO' (First In First Out).\n",
    "                         # If None, the eviction policy will not be applied.\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to choose a diamond ring?\n",
      "При выборе кольца с бриллиантом важно обратить внимание на несколько ключевых моментов, о которых я с удовольствием расскажу:\n",
      "\n",
      "1.  **\"4 С\" (4 C's):** Это международный стандарт оценки бриллиантов:\n",
      "    * **Carat (Каратность):** Вес камня. Чем больше карат, тем выше цена.\n",
      "    * **Cut (Огранка):** Качество огранки влияет на то, как камень преломляет свет и \"играет\". Идеальная огранка - залог блеска.\n",
      "    * **Clarity (Чистота):** Наличие внутренних и внешних дефектов (включений). Чем чище, тем дороже.\n",
      "    * **Color (Цвет):** Бриллианты оцениваются по шкале от D (абсолютно бесцветный, самый дорогой) до Z (имеет заметный желтоватый оттенок).\n",
      "\n",
      "2.  **Металл оправы:** Выбирайте металл, который подходит вам по цвету и долговечности. Классикой являются белое золото, желтое золото или платина. Белое золото и платина лучше подчеркивают бесцветность бриллианта.\n",
      "\n",
      "3.  **Стиль:** Выберите стиль, который понравится будущей владелице: **солитер** (один крупный камень), **паве** (множество мелких камней), **трилогия** (три камня, символизирующие прошлое, настоящее и будущее).\n",
      "\n",
      "4.  **Сертификат:** Убедитесь, что бриллиант сопровождается сертификатом от признанных геммологических лабораторий, например, GIA, HRD или IGI. Это гарантирует качество и характеристики камня.\n",
      "\n",
      "Если вы ищете что-то конкретное, например, в определенной ценовой категории, пожалуйста, уточните, и я смогу предложить вам подходящие варианты из нашего каталога. \n"
     ]
    }
   ],
   "source": [
    "user_query = \"How to choose a diamond ring?\"\n",
    "print(user_query)\n",
    "\n",
    "products, context = extract(user_query)\n",
    "\n",
    "# Creating a dictionary to pass to the chain\n",
    "inputs = {\n",
    "    \"context\": context, # Passing the context\n",
    "    \"products\": products, # Passing the list of products\n",
    "    \"question\": user_query # Passing the query itself\n",
    "}\n",
    "\n",
    "# Running the chain\n",
    "response = llm_chain.run(inputs)\n",
    "\n",
    "# Outputting the response\n",
    "print(response)"
   ]
  }
 ]
}